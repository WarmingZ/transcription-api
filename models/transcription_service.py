"""
–û—Å–Ω–æ–≤–Ω–∏–π —Å–µ—Ä–≤—ñ—Å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ—ó —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –∞—É–¥—ñ–æ –∑ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ—é –∫–æ—Ä–µ–∫—Ü—ñ—î—é
"""

import os
import logging
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
import torch
import librosa
import soundfile as sf
import time
import hashlib
from functools import lru_cache

from .config import logger, LANGUAGE_TOOL_AVAILABLE, SUPPORTED_MODELS, QUANTIZED_MODELS, ENABLE_DIARIZATION, DIARIZATION_MAX_WORKERS
from .whisper_model import LocalWhisperModel
from .diarization import SimpleDiarizationService

class LocalTranscriptionService:
    """–°–µ—Ä–≤—ñ—Å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ—ó —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –∞—É–¥—ñ–æ –∑ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ—é –∫–æ—Ä–µ–∫—Ü—ñ—î—é"""
    
    def __init__(self, beam_size: int = 1, best_of: int = 1, word_timestamps: bool = True):
        self.models_loaded = False
        self.whisper_model = None
        self.diarization_service = None
        self.language_tool = None
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
        self.beam_size = beam_size
        self.best_of = best_of
        self.word_timestamps = word_timestamps
        
        # –ö–µ—à—É–≤–∞–Ω–Ω—è –∞—É–¥—ñ–æ –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
        self._audio_cache = {}
        self._cache_max_size = 15  # –ú–∞–∫—Å–∏–º—É–º 15 —Ñ–∞–π–ª—ñ–≤ –≤ –∫–µ—à—ñ (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è 14GB RAM)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ LanguageTool –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        if LANGUAGE_TOOL_AVAILABLE:
            try:
                import language_tool_python
                self.language_tool = language_tool_python.LanguageTool('uk-UA')
                logger.info("LanguageTool —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏")
            except Exception as e:
                logger.warning(f"LanguageTool –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π (–ø–æ—Ç—Ä—ñ–±–µ–Ω Java): {e}")
                self.language_tool = None
        else:
            logger.info("LanguageTool –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–π - –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è –≤–∏–º–∫–Ω–µ–Ω–∞")
        
    def load_models(self, model_size: str = "auto") -> bool:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –º–æ–¥–µ–ª—ñ"""
        try:
            logger.info("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ª–æ–∫–∞–ª—å–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π...")
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ä–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞ 8GB RAM + 4 CPU AMD)
            if model_size == "auto":
                # –î–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–≤–∏—á–∞–π–Ω—ñ –º–æ–¥–µ–ª—ñ (distil —Ç—ñ–ª—å–∫–∏ –¥–ª—è –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó)
                try:
                    import psutil
                    memory_gb = psutil.virtual_memory().total / (1024**3)
                    cpu_count = psutil.cpu_count()
                    
                    if device == "cuda":
                        # –î–ª—è GPU –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –±—ñ–ª—å—à—ñ –º–æ–¥–µ–ª—ñ
                        try:
                            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                            
                            if gpu_memory >= 8:  # 8GB+ GPU
                                model_size = "medium"  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è GPU
                                logger.info(f"üöÄ GPU {gpu_memory:.1f}GB + RAM {memory_gb:.1f}GB - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è medium –º–æ–¥–µ–ª—å")
                            elif gpu_memory >= 4:  # 4GB+ GPU
                                model_size = "small"  # –ë–µ–∑–ø–µ—á–Ω–æ –¥–ª—è GPU
                                logger.info(f"üöÄ GPU {gpu_memory:.1f}GB + RAM {memory_gb:.1f}GB - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è small –º–æ–¥–µ–ª—å")
                            else:
                                model_size = "base"  # –î–ª—è –º–∞–ª–∏—Ö GPU
                                logger.info(f"üöÄ GPU {gpu_memory:.1f}GB + RAM {memory_gb:.1f}GB - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è base –º–æ–¥–µ–ª—å")
                        except:
                            model_size = "small"  # Fallback –¥–ª—è GPU
                    else:
                        # –î–ª—è CPU –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ quantized –º–æ–¥–µ–ª—ñ (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è ChatGPT)
                        if memory_gb >= 12 and cpu_count >= 8:
                            model_size = "medium"  # medium + int8 = quantized - –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –ø–æ—Ç—É–∂–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
                            logger.info(f"üöÄ –ü–æ—Ç—É–∂–Ω–∏–π —Å–µ—Ä–≤–µ—Ä {memory_gb:.1f}GB RAM + {cpu_count} CPU - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è medium –º–æ–¥–µ–ª—å (quantized)")
                        elif memory_gb >= 8 and cpu_count >= 6:
                            model_size = "small"  # small + int8 = quantized - –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è CPU
                            logger.info(f"üöÄ –°–µ—Ä–≤–µ—Ä {memory_gb:.1f}GB RAM + {cpu_count} CPU - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è small –º–æ–¥–µ–ª—å (quantized)")
                        elif memory_gb >= 6:
                            model_size = "base"  # base + int8 = quantized
                            logger.info(f"üíæ –°–µ—Ä–≤–µ—Ä {memory_gb:.1f}GB RAM - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è base –º–æ–¥–µ–ª—å (quantized)")
                        else:
                            model_size = "tiny"  # tiny + int8 = quantized –¥–ª—è –º–∞–ª–∏—Ö —Å–µ—Ä–≤–µ—Ä—ñ–≤
                            logger.info(f"üíæ –°–µ—Ä–≤–µ—Ä {memory_gb:.1f}GB RAM - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è tiny –º–æ–¥–µ–ª—å (quantized)")
                except:
                    model_size = "base"
            elif model_size not in SUPPORTED_MODELS:
                # –Ø–∫—â–æ –≤–∫–∞–∑–∞–Ω–æ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π —Ä–æ–∑–º—ñ—Ä, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ base
                logger.warning(f"–ù–µ–≤—ñ–¥–æ–º–∏–π —Ä–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ: {model_size}, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è base")
                model_size = "base"
            # –ü–æ–∫–∞–∑—É—î–º–æ —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è quantized –º–æ–¥–µ–ª—å
            logger.info(f"–û–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏: {model_size} (quantized –∑ int8)")
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ Whisper –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º –Ω–∞ —Å–µ—Ä–≤—ñ—Å –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è
            self.whisper_model = LocalWhisperModel(model_size=model_size, device=device, transcription_service=self)
            if not self.whisper_model.load_model():
                return False
            
            # –î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è –±—É–¥–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –ø–æ—Ç—Ä–µ–±—ñ (lazy loading)
            # self.diarization_service = SimpleDiarizationService(self)  # –í–∏–¥–∞–ª–µ–Ω–æ –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
            
            self.models_loaded = True
            logger.info("–í—Å—ñ –º–æ–¥–µ–ª—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ —É—Å–ø—ñ—à–Ω–æ")
            return True
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π: {e}")
            return False
    
    def _load_audio_cached(self, audio_path: str) -> Tuple[np.ndarray, int]:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∞—É–¥—ñ–æ –∑ –∫–µ—à—É –∞–±–æ –∑ –¥–∏—Å–∫—É"""
        try:
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ö–µ—à —Ñ–∞–π–ª—É –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è
            file_stat = os.stat(audio_path)
            file_hash = f"{audio_path}_{file_stat.st_size}_{file_stat.st_mtime}"
            
            if file_hash in self._audio_cache:
                logger.debug(f"–ê—É–¥—ñ–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ –∫–µ—à—É: {audio_path}")
                return self._audio_cache[file_hash]
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑ –¥–∏—Å–∫—É —Ç–∞ –∫–µ—à—É—î–º–æ
            logger.debug(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∞—É–¥—ñ–æ –∑ –¥–∏—Å–∫—É: {audio_path}")
            audio, sr = librosa.load(audio_path, sr=16000, mono=True)
            
            # –î–æ–¥–∞—î–º–æ –≤ –∫–µ—à (–∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º —Ä–æ–∑–º—ñ—Ä—É)
            if len(self._audio_cache) >= self._cache_max_size:
                # –í–∏–¥–∞–ª—è—î–º–æ –Ω–∞–π—Å—Ç–∞—Ä—ñ—à–∏–π –µ–ª–µ–º–µ–Ω—Ç
                oldest_key = next(iter(self._audio_cache))
                del self._audio_cache[oldest_key]
            
            self._audio_cache[file_hash] = (audio, sr)
            return audio, sr
            
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –∫–µ—à—É–≤–∞–Ω–Ω—è –∞—É–¥—ñ–æ: {e}, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑ –¥–∏—Å–∫—É")
            audio, sr = librosa.load(audio_path, sr=16000, mono=True)
            return audio, sr
    
    def _correct_text(self, text: str, language: str) -> str:
        """–û—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è —Ç–µ–∫—Å—Ç—É –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏"""
        if not text or language != "uk" or not self.language_tool:
            return text
        
        try:
            # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø–æ–º–∏–ª–∫–∏ —á–µ—Ä–µ–∑ LanguageTool
            matches = self.language_tool.check(text)
            import language_tool_python
            corrected_text = language_tool_python.utils.correct(text, matches)
            
            if corrected_text != text:
                logger.debug("–ó–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–æ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—É –∫–æ—Ä–µ–∫—Ü—ñ—é")
            
            return corrected_text
            
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ—ó –∫–æ—Ä–µ–∫—Ü—ñ—ó: {e}")
            return text
    
    def _correct_text_batch(self, texts: List[str], language: str) -> List[str]:
        """–ü–∞–∫–µ—Ç–Ω–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ"""
        if not texts or language != "uk" or not self.language_tool:
            return texts
        
        try:
            # –û–±'—î–¥–Ω—É—î–º–æ –≤—Å—ñ —Ç–µ–∫—Å—Ç–∏ –≤ –æ–¥–∏–Ω –¥–ª—è –∫–æ—Ä–µ–∫—Ü—ñ—ó
            combined_text = " ".join(texts)
            matches = self.language_tool.check(combined_text)
            import language_tool_python
            corrected_text = language_tool_python.utils.correct(combined_text, matches)
            
            # –†–æ–∑–¥—ñ–ª—è—î–º–æ –Ω–∞–∑–∞–¥ –Ω–∞ –æ–∫—Ä–µ–º—ñ —Ç–µ–∫—Å—Ç–∏ (—Å–ø—Ä–æ—â–µ–Ω–æ)
            corrected_texts = corrected_text.split(" ")
            
            # –Ø–∫—â–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ —Å–ø—ñ–≤–ø–∞–¥–∞—î, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Ç–µ–∫—Å—Ç–∏
            if len(corrected_texts) != len(texts):
                logger.warning("–ü–∞–∫–µ—Ç–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è –∑–º—ñ–Ω–∏–ª–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Ç–µ–∫—Å—Ç–∏")
                return texts
            
            return corrected_texts
            
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ—ó –∫–æ—Ä–µ–∫—Ü—ñ—ó: {e}")
            return texts
    
    def transcribe_simple(self, audio_path: str, language: str = "uk", model_size: str = "auto", use_parallel: bool = False, force_no_chunks: bool = True) -> Dict[str, Any]:
        """–®–≤–∏–¥–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∑ –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–º –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏–º –æ–±—Ä–æ–±–ª–µ–Ω–Ω—è–º"""
        if not self.models_loaded:
            raise RuntimeError("–ú–æ–¥–µ–ª—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–º—ñ–Ω–∏—Ç–∏ –º–æ–¥–µ–ª—å
        if model_size != "auto" and model_size != self.whisper_model.model_size:
            logger.info(f"üîÑ –ó–º—ñ–Ω–∞ –º–æ–¥–µ–ª—ñ –∑ {self.whisper_model.model_size} –Ω–∞ {model_size}")
            if not self.load_models(model_size):
                logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å {model_size}, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø–æ—Ç–æ—á–Ω–∞")
        
        start_time = time.time()
        try:
            logger.info(f"–ü–æ—á–∞—Ç–æ–∫ —à–≤–∏–¥–∫–æ—ó —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –∑ faster-whisper (–º–æ–¥–µ–ª—å: {self.whisper_model.model_size})...")
            
            # –ó–∞–≤–∂–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—É –æ–±—Ä–æ–±–∫—É (—á–∞–Ω–∫–∏ –≤–∏–º–∫–Ω–µ–Ω–æ)
            logger.info("üö´ –ß–∞–Ω–∫–∏ –≤–∏–º–∫–Ω–µ–Ω–æ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞")
            transcription_result = self.whisper_model.transcribe(audio_path, language)
            
            # –û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ—é –∫–æ—Ä–µ–∫—Ü—ñ—î—é
            processed_result = self._process_simple_results(transcription_result, language)
            
            elapsed_time = time.time() - start_time
            logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
            return processed_result
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó: {e}")
            raise
    
    def transcribe_with_diarization(self, audio_path: str, language: str = "uk", model_size: str = "auto", use_parallel: bool = False, force_no_chunks: bool = True) -> Dict[str, Any]:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é (–û–ø–µ—Ä–∞—Ç–æ—Ä/–ö–ª—ñ—î–Ω—Ç) –∑ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—é –æ–±—Ä–æ–±–∫–æ—é"""
        if not self.models_loaded:
            raise RuntimeError("–ú–æ–¥–µ–ª—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–º—ñ–Ω–∏—Ç–∏ –º–æ–¥–µ–ª—å
        if model_size != "auto" and model_size != self.whisper_model.model_size:
            logger.info(f"üîÑ –ó–º—ñ–Ω–∞ –º–æ–¥–µ–ª—ñ –∑ {self.whisper_model.model_size} –Ω–∞ {model_size}")
            if not self.load_models(model_size):
                logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å {model_size}, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø–æ—Ç–æ—á–Ω–∞")
        
        # Lazy loading –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó - —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –ø–æ—Ç—Ä–µ–±—ñ
        if not ENABLE_DIARIZATION:
            logger.warning("‚ö†Ô∏è –î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è –≤—ñ–¥–∫–ª—é—á–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó")
            raise RuntimeError("–î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è –≤—ñ–¥–∫–ª—é—á–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó")
        
        if self.diarization_service is None:
            logger.info("üîß –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (lazy loading)...")
            self.diarization_service = SimpleDiarizationService(self)
        
        start_time = time.time()
        try:
            logger.info("–ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é...")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ñ–∞–π–ª —Ç–∞ –æ—Ç—Ä–∏–º—É—î–º–æ –∞—É–¥—ñ–æ –º–∞—Å–∏–≤ –Ω–∞–ø—Ä—è–º—É
            audio, sr = self.whisper_model._convert_to_optimal_format(audio_path)
            logger.info(f"üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–æ –∞—É–¥—ñ–æ: {len(audio)} –∑—Ä–∞–∑–∫—ñ–≤, {sr}Hz")
            
            # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∞—É–¥—ñ–æ
            audio_duration = len(audio) / sr
            logger.info(f"üìä –ê—É–¥—ñ–æ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å={audio_duration:.2f}—Å, —á–∞—Å—Ç–æ—Ç–∞={sr}Hz, –∑—Ä–∞–∑–∫—ñ–≤={len(audio)}")
            logger.info(f"üìä –ü–µ—Ä—à—ñ 0.5—Å –∞—É–¥—ñ–æ: min={audio[:int(0.5*sr)].min():.4f}, max={audio[:int(0.5*sr)].max():.4f}, rms={np.sqrt(np.mean(audio[:int(0.5*sr)]**2)):.4f}")
            
            # –†–æ–±–∏–º–æ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—é –∑ –∞—É–¥—ñ–æ –º–∞—Å–∏–≤–æ–º
            speaker_segments = self.diarization_service.process_audio_array(audio, sr)
            
            if not speaker_segments:
                logger.warning("–î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è –Ω–µ –∑–Ω–∞–π—à–ª–∞ —Å–µ–≥–º–µ–Ω—Ç–∏, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–æ—Å—Ç—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é")
                return self.transcribe_simple(audio_path, language, use_parallel)
            
            if use_parallel and len(speaker_segments) > 1:
                # –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ ProcessPoolExecutor
                logger.info(f"–ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ {len(speaker_segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó...")
                processed_segments = self._process_diarization_segments_parallel(
                    audio, sr, speaker_segments, language
                )
            else:
                # –ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞ (fallback)
                processed_segments = self._process_diarization_segments_sequential(
                    audio, sr, speaker_segments, language
                )
            
            # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–∏–∫—Ç–æ—Ä–∞—Ö
            speakers_stats = {}
            full_text = ""
            
            # –°–æ—Ä—Ç—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑–∞ —á–∞—Å–æ–º –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫—É
            processed_segments.sort(key=lambda x: x["start"])
            
            for segment in processed_segments:
                speaker = segment["speaker"]
                if speaker not in speakers_stats:
                    speakers_stats[speaker] = {
                        "speaker": speaker,
                        "segments_count": 0,
                        "total_duration": 0,
                        "first_segment": segment["start"],
                        "last_segment": segment["end"]
                    }
                
                speakers_stats[speaker]["segments_count"] += 1
                speakers_stats[speaker]["total_duration"] += segment["duration"]
                speakers_stats[speaker]["last_segment"] = max(speakers_stats[speaker]["last_segment"], segment["end"])
                
                # –î–æ–¥–∞—î–º–æ —á–∞—Å –¥–æ —Ç–µ–∫—Å—Ç—É –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è
                time_info = f"[{segment['start']:.1f}s-{segment['end']:.1f}s]"
                full_text += f"{time_info} [{speaker}]: {segment['text']}\n"
            
            result = {
                "text": full_text.strip(),
                "segments": processed_segments,
                "speakers": list(speakers_stats.values()),
                "duration": max([s["end"] for s in processed_segments]) if processed_segments else 0,
                "language": language,
                "diarization_type": "simple_alternating_parallel" if use_parallel else "simple_alternating"
            }
            
            elapsed_time = time.time() - start_time
            logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(processed_segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
            return result
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –∑ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—î—é: {e}")
            raise
    
    def _process_diarization_segments_parallel(self, audio: np.ndarray, sr: int, 
                                             speaker_segments: List[Dict[str, Any]], 
                                             language: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑ multiprocessing (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è 8 CPU)"""
        try:
            from concurrent.futures import ProcessPoolExecutor
            import os
            
            # –î–∏–Ω–∞–º—ñ—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –ø—Ä–æ—Ü–µ—Å—ñ–≤ (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞ 14GB RAM + 8 CPU)
            cpu_count = os.cpu_count()
            max_workers = min(DIARIZATION_MAX_WORKERS, len(speaker_segments), cpu_count - 1)  # –ó–∞–ª–∏—à–∞—î–º–æ 1 CPU –¥–ª—è —Å–∏—Å—Ç–µ–º–∏
            logger.info(f"üöÄ –°–µ—Ä–≤–µ—Ä {cpu_count} CPU + 14GB RAM - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è {max_workers} –ø—Ä–æ—Ü–µ—Å—ñ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó")
            
            # –Ø–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –º–∞–ª–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—É –æ–±—Ä–æ–±–∫—É
            if len(speaker_segments) <= 2 or max_workers <= 1:
                logger.info("–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞ (–º–∞–ª–æ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤)")
                return self._process_diarization_segments_sequential(audio, sr, speaker_segments, language)
            
            # –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –∑ ProcessPoolExecutor
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–¥–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç—É
                futures = [
                    executor.submit(
                        self._process_single_diarization_segment_worker_optimized,
                        audio, sr, segment, language
                    )
                    for segment in speaker_segments
                ]
                
                # –ß–µ–∫–∞—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –≤—Å—ñ—Ö –∑–∞–≤–¥–∞–Ω—å
                processed_segments = []
                for i, future in enumerate(futures):
                    try:
                        result = future.result(timeout=300)  # 5 —Ö–≤–∏–ª–∏–Ω timeout
                        if result:
                            processed_segments.append(result)
                        
                        # –õ–æ–≥—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å –∫–æ–∂–Ω—ñ 3 —Å–µ–≥–º–µ–Ω—Ç–∏
                        if (i + 1) % 3 == 0:
                            logger.info(f"–ü–∞—Ä–∞–ª–µ–ª—å–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ {i + 1}/{len(speaker_segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
                            
                    except Exception as e:
                        logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Å–µ–≥–º–µ–Ω—Ç—É {speaker_segments[i].get('speaker', 'unknown')}: {e}")
                        continue
            
            logger.info(f"–ü–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(processed_segments)}/{len(speaker_segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
            return processed_segments
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó: {e}")
            # Fallback –¥–æ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏
            return self._process_diarization_segments_sequential(audio, sr, speaker_segments, language)
    
    @staticmethod
    def _process_single_diarization_segment_worker_optimized(audio: np.ndarray, sr: int,
                                                           speaker_info: Dict[str, Any], 
                                                           language: str) -> Optional[Dict[str, Any]]:
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π worker –¥–ª—è multiprocessing (—Å—Ç–∞—Ç–∏—á–Ω–∏–π –º–µ—Ç–æ–¥)"""
        try:
            # –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ç—É—Ç, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø—Ä–æ–±–ª–µ–º –∑ multiprocessing
            from faster_whisper import WhisperModel
            import torch
            
            start_time = speaker_info["start"]
            end_time = speaker_info["end"]
            speaker = speaker_info["speaker"]
            
            # –í–∏—Ç—è–≥—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç –∞—É–¥—ñ–æ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ–≥–æ –º–∞—Å–∏–≤—É
            start_sample = int(start_time * sr)
            end_sample = int(end_time * sr)
            segment_audio = audio[start_sample:end_sample]
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π —Ç–∞ compute_type (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞ 14GB RAM + 8 CPU)
            device = "cuda" if torch.cuda.is_available() else "cpu"
            if device == "cpu":
                try:
                    import psutil
                    memory_gb = psutil.virtual_memory().total / (1024**3)
                    cpu_count = psutil.cpu_count()
                    
                    if memory_gb >= 14 and cpu_count >= 8:
                        compute_type = "int8_float16"  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –≤–∞—à–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
                    elif memory_gb >= 8:
                        compute_type = "int8_float16"  # –®–≤–∏–¥—à–µ –Ω—ñ–∂ int8
                    else:
                        compute_type = "int8"  # –ï–∫–æ–Ω–æ–º–Ω—ñ—à–µ –ø–æ –ø–∞–º'—è—Ç—ñ
                except:
                    compute_type = "int8"
            else:
                compute_type = "float16"
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å –≤ worker –ø—Ä–æ—Ü–µ—Å—ñ (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è 8 CPU)
            model = WhisperModel("base", device=device, compute_type=compute_type, cpu_threads=2)
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç –∑ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            segments, info = model.transcribe(
                segment_audio,
                language=language,
                beam_size=1,  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
                word_timestamps=False,  # –®–≤–∏–¥—à–µ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                vad_filter=False,  # –®–≤–∏–¥—à–µ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                temperature=0.0,
                best_of=1,
            )
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            segment_text = ""
            for segment in segments:
                segment_text += segment.text + " "
            
            if segment_text.strip():
                return {
                    "start": start_time,
                    "end": end_time,
                    "text": segment_text.strip(),
                    "speaker": speaker,
                    "duration": end_time - start_time
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ worker –æ–±—Ä–æ–±–∫–∏ —Å–µ–≥–º–µ–Ω—Ç—É {speaker}: {e}")
            return None
    
    def _process_single_diarization_segment_optimized(self, audio: np.ndarray, sr: int,
                                                     speaker_info: Dict[str, Any], 
                                                     language: str) -> Optional[Dict[str, Any]]:
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç—É –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —ñ—Å–Ω—É—é—á—É –º–æ–¥–µ–ª—å)"""
        try:
            start_time = speaker_info["start"]
            end_time = speaker_info["end"]
            speaker = speaker_info["speaker"]
            
            # –í–∏—Ç—è–≥—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç –∞—É–¥—ñ–æ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ–≥–æ –º–∞—Å–∏–≤—É
            start_sample = int(start_time * sr)
            end_sample = int(end_time * sr)
            segment_audio = audio[start_sample:end_sample]
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ñ—Å–Ω—É—é—á—É –º–æ–¥–µ–ª—å –∑–∞–º—ñ—Å—Ç—å —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ—ó
            segments, info = self.whisper_model.model.transcribe(
                segment_audio,  # –ü–µ—Ä–µ–¥–∞—î–º–æ –º–∞—Å–∏–≤ –Ω–∞–ø—Ä—è–º—É
                language=language,
                beam_size=self.beam_size,
                word_timestamps=False,  # –®–≤–∏–¥—à–µ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                vad_filter=False,  # –®–≤–∏–¥—à–µ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                temperature=0.0,
                best_of=1,
            )
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            segment_text = ""
            for segment in segments:
                segment_text += segment.text + " "
            
            if segment_text.strip():
                # –û—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è
                if language == "uk":
                    segment_text = self._correct_text(segment_text.strip(), language)
                
                return {
                    "start": start_time,
                    "end": end_time,
                    "text": segment_text.strip(),
                    "speaker": speaker,
                    "duration": end_time - start_time
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Å–µ–≥–º–µ–Ω—Ç—É {speaker}: {e}")
            return None
    
    def _process_diarization_segments_sequential(self, audio: np.ndarray, sr: int,
                                               speaker_segments: List[Dict[str, Any]], 
                                               language: str) -> List[Dict[str, Any]]:
        """–ü–æ—Å–ª—ñ–¥–æ–≤–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (fallback)"""
        processed_segments = []
        
        for speaker_info in speaker_segments:
            try:
                result = self._process_single_diarization_segment_optimized(audio, sr, speaker_info, language)
                if result:
                    processed_segments.append(result)
            except Exception as e:
                logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Å–µ–≥–º–µ–Ω—Ç—É {speaker_info.get('speaker', 'unknown')}: {e}")
                continue
        
        return processed_segments
    
    def _process_single_diarization_segment(self, audio: np.ndarray, sr: int,
                                          speaker_info: Dict[str, Any], 
                                          language: str) -> Optional[Dict[str, Any]]:
        """–û–±—Ä–æ–±–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç—É –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó (fallback –∑ —Ç–∏–º—á–∞—Å–æ–≤–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏)"""
        try:
            start_time = speaker_info["start"]
            end_time = speaker_info["end"]
            speaker = speaker_info["speaker"]
            
            # –í–∏—Ç—è–≥—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç –∞—É–¥—ñ–æ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ–≥–æ –º–∞—Å–∏–≤—É
            start_sample = int(start_time * sr)
            end_sample = int(end_time * sr)
            segment_audio = audio[start_sample:end_sample]
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç—É (fallback –º–µ—Ç–æ–¥)
            segment_path = f"temp_segment_{start_time:.1f}_{end_time:.1f}.wav"
            sf.write(segment_path, segment_audio, sr, format='WAV', subtype='PCM_16')
            
            try:
                # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç –∑ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                segments, info = self.whisper_model.model.transcribe(
                    segment_path,
                    language=language,
                    beam_size=self.beam_size,
                    word_timestamps=False,  # –®–≤–∏–¥—à–µ
                    vad_filter=False,  # –®–≤–∏–¥—à–µ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
                    temperature=0.0,
                    best_of=1,
                )
                
                # –û–±—Ä–æ–±–ª—è—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                segment_text = ""
                for segment in segments:
                    segment_text += segment.text + " "
                
                if segment_text.strip():
                    # –û—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è
                    if language == "uk":
                        segment_text = self._correct_text(segment_text.strip(), language)
                    
                    return {
                        "start": start_time,
                        "end": end_time,
                        "text": segment_text.strip(),
                        "speaker": speaker,
                        "duration": end_time - start_time
                    }
                
                return None
                
            finally:
                # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
                try:
                    os.unlink(segment_path)
                except:
                    pass
                    
        except Exception as e:
            logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Å–µ–≥–º–µ–Ω—Ç—É {speaker}: {e}")
            return None
    
    def _process_simple_results(self, transcription_result: Dict[str, Any], language: str) -> Dict[str, Any]:
        """–û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø—Ä–æ—Å—Ç–æ—ó —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –∑ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ—é –∫–æ—Ä–µ–∫—Ü—ñ—î—é"""
        try:
            if not transcription_result or not transcription_result.get("text"):
                return {
                    "text": "–¢–µ–∫—Å—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - –∞—É–¥—ñ–æ –º–æ–∂–µ –±—É—Ç–∏ –±–µ–∑ –º–æ–≤–ª–µ–Ω–Ω—è –∞–±–æ –∑–∞–Ω–∞–¥—Ç–æ —Ç–∏—Ö–∏–º",
                    "segments": [],
                    "duration": 0,
                    "language": language
                }
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç
            text = transcription_result.get("text", "").strip()
            
            # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ —Ç–µ–∫—Å—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–æ–¥—É—î—Ç—å—Å—è –≤ UTF-8
            if isinstance(text, bytes):
                text = text.decode('utf-8', errors='ignore')
            
            # –û—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏
            if language == "uk":
                text = self._correct_text(text, language)
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –∑ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–æ—é –∫–æ—Ä–µ–∫—Ü—ñ—î—é
            segments = []
            segment_texts = []
            
            # –°–ø–æ—á–∞—Ç–∫—É –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —Ç–µ–∫—Å—Ç–∏ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ—ó –∫–æ—Ä–µ–∫—Ü—ñ—ó
            for segment in transcription_result.get("segments", []):
                segment_text = segment.get("text", "").strip()
                
                # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ —Ç–µ–∫—Å—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–æ–¥—É—î—Ç—å—Å—è –≤ UTF-8
                if isinstance(segment_text, bytes):
                    segment_text = segment_text.decode('utf-8', errors='ignore')
                
                segment_texts.append(segment_text)
            
            # –ü–∞–∫–µ—Ç–Ω–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ—ñ—á–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞)
            if language == "uk" and segment_texts:
                try:
                    corrected_texts = self._correct_text_batch(segment_texts, language)
                    if len(corrected_texts) == len(segment_texts):
                        segment_texts = corrected_texts
                except Exception as e:
                    logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ—ó –∫–æ—Ä–µ–∫—Ü—ñ—ó: {e}")
            
            # –§–æ—Ä–º—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—ñ —Å–µ–≥–º–µ–Ω—Ç–∏
            for i, segment in enumerate(transcription_result.get("segments", [])):
                segment_text = segment_texts[i] if i < len(segment_texts) else segment.get("text", "").strip()
                
                segments.append({
                    "start": segment.get("start", 0),
                    "end": segment.get("end", 0),
                    "text": segment_text
                })
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å
            duration = transcription_result.get("duration", 0)
            
            return {
                "text": text,
                "segments": segments,
                "duration": duration,
                "language": language
            }
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤: {e}")
            return {
                "text": "–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó",
                "segments": [],
                "duration": 0,
                "language": language
            }
