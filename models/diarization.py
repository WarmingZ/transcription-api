"""
–ü—Ä–æ—Å—Ç–∏–π —Å–µ—Ä–≤—ñ—Å –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑ —á–µ—Ä–≥—É–≤–∞–Ω–Ω—è–º —Ä–æ–ª–µ–π –û–ø–µ—Ä–∞—Ç–æ—Ä/–ö–ª—ñ—î–Ω—Ç
"""

import logging
import numpy as np
from typing import Dict, Any, List, Tuple
import librosa
import webrtcvad

from .config import logger

class SimpleDiarizationService:
    """–ü—Ä–æ—Å—Ç–∏–π —Å–µ—Ä–≤—ñ—Å –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∑ —á–µ—Ä–≥—É–≤–∞–Ω–Ω—è–º —Ä–æ–ª–µ–π –û–ø–µ—Ä–∞—Ç–æ—Ä/–ö–ª—ñ—î–Ω—Ç"""
    
    def __init__(self, transcription_service=None):
        self.vad = webrtcvad.Vad(2)  # –ê–≥—Ä–µ—Å–∏–≤–Ω—ñ—Å—Ç—å VAD (0-3, –¥–µ 3 –Ω–∞–π–∞–≥—Ä–µ—Å–∏–≤–Ω—ñ—à–∞)
        self.transcription_service = transcription_service  # –ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤–Ω–∏–π —Å–µ—Ä–≤—ñ—Å –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è
        
    def _detect_speech_segments(self, audio_path: str, min_silence_duration: float = 0.5) -> List[Tuple[float, float]]:
        """–í–∏—è–≤–ª—è—î —Å–µ–≥–º–µ–Ω—Ç–∏ –º–æ–≤–ª–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é WebRTC VAD –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –ª–æ–≥—ñ–∫–æ—é"""
        try:
            logger.info(f"–ê–Ω–∞–ª—ñ–∑ –º–æ–≤–ª–µ–Ω–Ω—è –¥–ª—è {audio_path}...")
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∞—É–¥—ñ–æ —è–∫ 16kHz mono PCM –∑ –∫–µ—à—É–≤–∞–Ω–Ω—è–º
            if self.transcription_service:
                logger.info(f"üîç VAD –∞–Ω–∞–ª—ñ–∑ —Ñ–∞–π–ª—É: {audio_path}")
                audio, sr = self.transcription_service._load_audio_cached(audio_path)
            else:
                logger.info(f"üîç VAD –∞–Ω–∞–ª—ñ–∑ —Ñ–∞–π–ª—É (–±–µ–∑ –∫–µ—à—É): {audio_path}")
                audio, sr = librosa.load(audio_path, sr=16000, mono=True)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ int16 –¥–ª—è WebRTC VAD
            audio_int16 = (audio * 32767).astype(np.int16)
            
            # –ü–æ–∫—Ä–∞—â–µ–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è VAD
            frame_duration = 20  # –º—Å (–∑–º–µ–Ω—à–µ–Ω–æ –¥–ª—è –∫—Ä–∞—â–æ—ó —á—É—Ç–ª–∏–≤–æ—Å—Ç—ñ)
            frame_size = int(16000 * frame_duration / 1000)
            
            speech_segments = []
            current_segment_start = None
            in_speech = False
            silence_frames = 0
            min_silence_frames = int(min_silence_duration * 1000 / frame_duration)  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–∞–¥—Ä—ñ–≤ —Ç–∏—à—ñ
            
            logger.info(f"VAD –ø–∞—Ä–∞–º–µ—Ç—Ä–∏: frame_duration={frame_duration}ms, min_silence_frames={min_silence_frames}")
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –∞—É–¥—ñ–æ –∫–∞–¥—Ä–∞–º–∏
            for i in range(0, len(audio_int16) - frame_size, frame_size):
                frame = audio_int16[i:i + frame_size]
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –º–æ–≤–ª–µ–Ω–Ω—è –≤ –∫–∞–¥—Ä—ñ
                is_speech = self.vad.is_speech(frame.tobytes(), 16000)
                timestamp = i / 16000.0  # —á–∞—Å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
                
                if is_speech:
                    if not in_speech:
                        # –ü–æ—á–∞—Ç–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—É –º–æ–≤–ª–µ–Ω–Ω—è
                        current_segment_start = timestamp
                        in_speech = True
                        silence_frames = 0
                        logger.debug(f"–ü–æ—á–∞—Ç–æ–∫ –º–æ–≤–ª–µ–Ω–Ω—è –Ω–∞ {timestamp:.2f}—Å")
                    else:
                        silence_frames = 0  # –°–∫–∏–¥–∞—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫ —Ç–∏—à—ñ
                else:
                    if in_speech:
                        silence_frames += 1
                        # –ö—ñ–Ω–µ—Ü—å —Å–µ–≥–º–µ–Ω—Ç—É –º–æ–≤–ª–µ–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –ø—ñ—Å–ª—è –¥–æ—Å—Ç–∞—Ç–Ω—å–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–∞–¥—Ä—ñ–≤ —Ç–∏—à—ñ
                        if silence_frames >= min_silence_frames:
                            segment_duration = timestamp - current_segment_start
                            # –ó–º–µ–Ω—à–µ–Ω–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—É —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—É –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑
                            if segment_duration >= 0.1:  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ 0.3 –¥–æ 0.1 —Å–µ–∫—É–Ω–¥–∏
                                speech_segments.append((current_segment_start, timestamp))
                                logger.debug(f"–°–µ–≥–º–µ–Ω—Ç –º–æ–≤–ª–µ–Ω–Ω—è: {current_segment_start:.2f}—Å - {timestamp:.2f}—Å ({segment_duration:.2f}—Å)")
                            else:
                                logger.debug(f"–°–µ–≥–º–µ–Ω—Ç –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π ({segment_duration:.2f}—Å), –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ")
                            in_speech = False
                            silence_frames = 0
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç
            if in_speech and current_segment_start is not None:
                final_timestamp = len(audio_int16) / 16000.0
                segment_duration = final_timestamp - current_segment_start
                if segment_duration >= 0.1:  # –ó–º–µ–Ω—à–µ–Ω–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—É —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å
                    speech_segments.append((current_segment_start, final_timestamp))
                    logger.debug(f"–û—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç –º–æ–≤–ª–µ–Ω–Ω—è: {current_segment_start:.2f}—Å - {final_timestamp:.2f}—Å")
            
            logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(speech_segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –º–æ–≤–ª–µ–Ω–Ω—è")
            if speech_segments:
                logger.info(f"–ü–µ—Ä—à–∏–π —Å–µ–≥–º–µ–Ω—Ç: {speech_segments[0][0]:.2f}—Å - {speech_segments[0][1]:.2f}—Å")
                logger.info(f"–û—Å—Ç–∞–Ω–Ω—ñ–π —Å–µ–≥–º–µ–Ω—Ç: {speech_segments[-1][0]:.2f}—Å - {speech_segments[-1][1]:.2f}—Å")
            
            return speech_segments
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤–∏—è–≤–ª–µ–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –º–æ–≤–ª–µ–Ω–Ω—è: {e}")
            return []
    
    def _merge_close_segments(self, segments: List[Tuple[float, float]], max_gap: float = 1.0) -> List[Tuple[float, float]]:
        """–û–±'—î–¥–Ω—É—î –±–ª–∏–∑—å–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏ –º–æ–≤–ª–µ–Ω–Ω—è –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –æ–±—Ä–æ–±–∫–æ—é –ø–æ—á–∞—Ç–∫—É"""
        if not segments:
            return []
        
        # –î–æ–¥–∞—î–º–æ –±—É—Ñ–µ—Ä –Ω–∞ –ø–æ—á–∞—Ç–æ–∫ —Ñ–∞–π–ª—É –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø–µ—Ä—à–∏—Ö —Å–ª—ñ–≤
        processed_segments = []
        for start, end in segments:
            # –†–æ–∑—à–∏—Ä—é—î–º–æ –ø–æ—á–∞—Ç–æ–∫ —Å–µ–≥–º–µ–Ω—Ç—É –Ω–∞ 0.2 —Å–µ–∫—É–Ω–¥–∏ –Ω–∞–∑–∞–¥ (–∞–ª–µ –Ω–µ –º–µ–Ω—à–µ 0)
            extended_start = max(0, start - 0.2)
            processed_segments.append((extended_start, end))
        
        merged = [processed_segments[0]]
        
        for current_start, current_end in processed_segments[1:]:
            last_start, last_end = merged[-1]
            
            # –Ø–∫—â–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –±–ª–∏–∑—å–∫–æ –æ–¥–∏–Ω –¥–æ –æ–¥–Ω–æ–≥–æ, –æ–±'—î–¥–Ω—É—î–º–æ —ó—Ö
            if current_start - last_end <= max_gap:
                merged[-1] = (last_start, current_end)
            else:
                merged.append((current_start, current_end))
        
        logger.info(f"–û–±'—î–¥–Ω–∞–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –≤ {len(merged)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
        return merged
    
    def assign_speakers(self, speech_segments: List[Tuple[float, float]]) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–∑–Ω–∞—á–∞—î —Ä–æ–ª—ñ –û–ø–µ—Ä–∞—Ç–æ—Ä/–ö–ª—ñ—î–Ω—Ç —á–µ—Ä–≥—É—é—á–∏ —ó—Ö"""
        if not speech_segments:
            return []
        
        speaker_assignments = []
        
        for i, (start, end) in enumerate(speech_segments):
            # –ß–µ—Ä–≥—É—î–º–æ —Ä–æ–ª—ñ: –ø–µ—Ä—à–∏–π —Å–µ–≥–º–µ–Ω—Ç - –û–ø–µ—Ä–∞—Ç–æ—Ä, –¥—Ä—É–≥–∏–π - –ö–ª—ñ—î–Ω—Ç, —ñ —Ç.–¥.
            speaker = "–û–ø–µ—Ä–∞—Ç–æ—Ä" if i % 2 == 0 else "–ö–ª—ñ—î–Ω—Ç"
            
            speaker_assignments.append({
                "speaker": speaker,
                "start": start,
                "end": end,
                "duration": end - start
            })
        
        return speaker_assignments
    
    def process_audio(self, audio_path: str) -> List[Dict[str, Any]]:
        """–ü–æ–≤–Ω–∏–π –ø—Ä–æ—Ü–µ—Å –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó –∞—É–¥—ñ–æ"""
        try:
            logger.info("–ü–æ—á–∞—Ç–æ–∫ –ø—Ä–æ—Å—Ç–æ—ó –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó...")
            
            # –í–∏—è–≤–ª—è—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –º–æ–≤–ª–µ–Ω–Ω—è
            speech_segments = self._detect_speech_segments(audio_path)
            
            if not speech_segments:
                logger.warning("–°–µ–≥–º–µ–Ω—Ç–∏ –º–æ–≤–ª–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                return []
            
            # –û–±'—î–¥–Ω—É—î–º–æ –±–ª–∏–∑—å–∫—ñ —Å–µ–≥–º–µ–Ω—Ç–∏
            merged_segments = self._merge_close_segments(speech_segments)
            
            # –ü—Ä–∏–∑–Ω–∞—á–∞—î–º–æ —Ä–æ–ª—ñ
            speaker_assignments = self.assign_speakers(merged_segments)
            
            logger.info(f"–î—ñ–∞—Ä–∏–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(speaker_assignments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤ –∑ —Ä–æ–ª—è–º–∏")
            return speaker_assignments
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –¥—ñ–∞—Ä–∏–∑–∞—Ü—ñ—ó: {e}")
            return []
